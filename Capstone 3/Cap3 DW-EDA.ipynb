{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments, GPT2Tokenizer, AutoModelWithLMHead, AutoTokenizer\n",
    "from transformers import pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('trump_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.date = pd.to_datetime(df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 56571 entries, 0 to 56570\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   id         56571 non-null  int64         \n",
      " 1   text       56571 non-null  object        \n",
      " 2   device     56571 non-null  object        \n",
      " 3   favorites  56571 non-null  int64         \n",
      " 4   retweets   56571 non-null  int64         \n",
      " 5   date       56571 non-null  datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(3), object(2)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@kennybud  If you love what you're doing, motivation is never a problem.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(df.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\"favorites\":\"likes\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>56571.0</td>\n",
       "      <td>7.987865e+17</td>\n",
       "      <td>3.826616e+17</td>\n",
       "      <td>1.698309e+09</td>\n",
       "      <td>4.606190e+17</td>\n",
       "      <td>7.471020e+17</td>\n",
       "      <td>1.193130e+18</td>\n",
       "      <td>1.347570e+18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>likes</th>\n",
       "      <td>56571.0</td>\n",
       "      <td>2.834955e+04</td>\n",
       "      <td>5.781564e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>1.000000e+01</td>\n",
       "      <td>1.640000e+02</td>\n",
       "      <td>4.393850e+04</td>\n",
       "      <td>1.869706e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>retweets</th>\n",
       "      <td>56571.0</td>\n",
       "      <td>8.618987e+03</td>\n",
       "      <td>1.330613e+04</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>5.900000e+01</td>\n",
       "      <td>3.450000e+03</td>\n",
       "      <td>1.301450e+04</td>\n",
       "      <td>4.088660e+05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            count          mean           std           min           25%  \\\n",
       "id        56571.0  7.987865e+17  3.826616e+17  1.698309e+09  4.606190e+17   \n",
       "likes     56571.0  2.834955e+04  5.781564e+04  0.000000e+00  1.000000e+01   \n",
       "retweets  56571.0  8.618987e+03  1.330613e+04  0.000000e+00  5.900000e+01   \n",
       "\n",
       "                   50%           75%           max  \n",
       "id        7.471020e+17  1.193130e+18  1.347570e+18  \n",
       "likes     1.640000e+02  4.393850e+04  1.869706e+06  \n",
       "retweets  3.450000e+03  1.301450e+04  4.088660e+05  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From 2009-05-04 18:54:00 to 2021-01-08 15:44:00\n"
     ]
    }
   ],
   "source": [
    "print(f\"From {min(df.date)} to {max(df.date)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['month'] = df.date.apply(lambda x: x.month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['year'] = df.date.apply(lambda x: x.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_per_year = df.groupby('year').agg('count')['id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='year', ylabel='id'>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.scatterplot(data=tweets_per_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of times Barack Obama or Hillary Clinton are mentioned in DJT Tweets\n",
      "2235 out of 56571\n"
     ]
    }
   ],
   "source": [
    "common_targets = ['obama', 'hillary', 'clinton', 'barrack']\n",
    "\n",
    "count = df.text.apply(lambda x: any(word in common_targets for word in x.lower().split())).sum()\n",
    "print(f\"Number of times Barack Obama or Hillary Clinton are mentioned in DJT Tweets\\n{count} out of {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10010"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.text.apply(lambda x: (\"RT\" in x[:3]))]['text'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32876"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.text.apply(lambda x: \"@\" in x)]['text'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find targets of @'s\n",
    "\n",
    "def at_target(text):\n",
    "    text = text.split()\n",
    "    targets = []\n",
    "    for word in text:\n",
    "        if word[0] == '@':\n",
    "            targets.append(word.strip(\":|.|\\\"\"))\n",
    "    if len(targets) == 0:\n",
    "        targets = ['None']\n",
    "    return targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                 (None, 26526)\n",
      "1     (@realDonaldTrump, 10195)\n",
      "2               (@FoxNews, 811)\n",
      "3            (@WhiteHouse, 788)\n",
      "4         (@foxandfriends, 660)\n",
      "5           (@BarackObama, 517)\n",
      "6         (@ApprenticeNBC, 346)\n",
      "7             (@TeamTrump, 315)\n",
      "8                   (@CNN, 314)\n",
      "9           (@IvankaTrump, 258)\n",
      "10          (@seanhannity, 256)\n",
      "11           (@MittRomney, 231)\n",
      "12                  (@GOP, 226)\n",
      "13        (@GOPChairwoman, 217)\n",
      "14       (@DonaldJTrumpJr, 204)\n",
      "15           (@DanScavino, 195)\n",
      "16        (@BreitbartNews, 190)\n",
      "17            (@EricTrump, 176)\n",
      "18           (@Jim_Jordan, 169)\n",
      "19              (@nytimes, 160)\n",
      "20         (@GreggJarrett, 140)\n",
      "21           (@megynkelly, 137)\n",
      "22             (@dbongino, 137)\n",
      "23               (@FLOTUS, 132)\n",
      "24             (@LouDobbs, 130)\n",
      "25        (@marklevinshow, 124)\n",
      "26      (@LindseyGrahamSC, 123)\n",
      "27                 (@OANN, 122)\n",
      "28                (@POTUS, 119)\n",
      "29        (@oreillyfactor, 118)\n",
      "dtype: object\n",
      "69378\n"
     ]
    }
   ],
   "source": [
    "tweet_targets = list(df.text.apply(lambda x: at_target(x)))\n",
    "\n",
    "target_count = Counter()\n",
    "for targets in tweet_targets:\n",
    "    for target in targets:\n",
    "        target_count[target]+=1\n",
    "\n",
    "print(pd.Series(target_count.most_common(30)))\n",
    "print(sum(target_count.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     47261\n",
       "2      6799\n",
       "3      1842\n",
       "4       466\n",
       "5       144\n",
       "6        34\n",
       "7        11\n",
       "8         6\n",
       "9         4\n",
       "11        4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(map(len, tweet_targets)).value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hashtags(input_text):\n",
    "    t = input_text.split()\n",
    "    tag_list = []\n",
    "    for word in t:\n",
    "        if word[0] == \"#\":\n",
    "            tag_list.append(word)\n",
    "    return tag_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Threads\"\n",
    "Tweets were limited to 140 characters and then changed to 280 characters. Because of their small character limit, users that wish to have longer messages must do so over multiple tweets - called threads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4262"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.apply(lambda x: x[-2:] == \"..\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text.apply(lambda x: x[-1:] == \")\").sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperlinks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets with hyperlinks is:\n",
      "11511 out of 56571 (20.3%)\n"
     ]
    }
   ],
   "source": [
    "count = df.text.apply(lambda x: \"https:\" in x).sum()\n",
    "\n",
    "print(f\"The number of tweets with hyperlinks is:\\n{count} out of {len(df.text)} ({round(count/len(df.text)*100,1)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Biden lied Pennsylvania! https://t.co/zmD8Ew0bmS'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample hyperlink\n",
    "df[df.text.apply(lambda x: \"https:\" in x)]['text'].iloc[30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFirst Method\\n# Make a method to strip out the hyperlinks\\ndef hyperlink_remover(input_text):\\n    t = input_text\\n    start = 0\\n    end = 0\\n    while start >= 0:\\n        start = t.find(\"https\")\\n        if start >= 0:\\n            if t[start:].find(\" \") >=0:\\n                end = t[start:].find(\" \") + start\\n            else:\\n                end = start+len(t[start:])\\n            t = t[:start]+t[end:]\\n    return t.strip()\\n    \\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "First Method\n",
    "# Make a method to strip out the hyperlinks\n",
    "def hyperlink_remover(input_text):\n",
    "    t = input_text\n",
    "    start = 0\n",
    "    end = 0\n",
    "    while start >= 0:\n",
    "        start = t.find(\"https\")\n",
    "        if start >= 0:\n",
    "            if t[start:].find(\" \") >=0:\n",
    "                end = t[start:].find(\" \") + start\n",
    "            else:\n",
    "                end = start+len(t[start:])\n",
    "            t = t[:start]+t[end:]\n",
    "    return t.strip()\n",
    "    \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a method to strip out the hyperlinks\n",
    "def hyperlink_remover(input_text):\n",
    "    t = input_text.split()\n",
    "    output_string = \"\"\n",
    "    for word in t:\n",
    "        if \"http\" in word:\n",
    "            start=word.find(\"https\")\n",
    "            s = \" \" + word[:start]\n",
    "            output_string += s\n",
    "            output_string = output_string.strip()\n",
    "        else:\n",
    "            output_string += (\" \" + word)\n",
    "            output_string = output_string.strip()\n",
    "    return output_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'blah pew pew pew asdasd qerw'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the method\n",
    "hyperlink_remover(\"https blah https pew pew pew asdasdhttps qerw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of tweets with hyperlinks is:\n",
      "0 out of 56571 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Test to make sure it gives the correct sum\n",
    "foo = df.text.apply(lambda x: hyperlink_remover(x))\n",
    "\n",
    "count = foo.apply(lambda x: \"https:\" in x).sum()\n",
    "\n",
    "print(f\"The number of tweets with hyperlinks is:\\n{count} out of {len(df.text)} ({round(count/len(df.text)*100,1)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: hyperlink_remover(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        Republicans and Democrats have both created ou...\n",
       "1        I was thrilled to be back in the Great city of...\n",
       "2        RT @CBS_Herridge: READ: Letter to surveillance...\n",
       "3        The Unsolicited Mail In Ballot Scam is a major...\n",
       "4        RT @MZHemingway: Very friendly telling of even...\n",
       "                               ...                        \n",
       "56566    RT @RandPaul: I don’t know why @JoeBiden think...\n",
       "56567    RT @EliseStefanik: President @realDonaldTrump ...\n",
       "56568    RT @TeamTrump: LIVE: Presidential Debate #Deba...\n",
       "56569    Just signed an order to support the workers of...\n",
       "56570    Suburban women want Safety &amp; Security. Joe...\n",
       "Name: text, Length: 56571, dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2Model were not initialized from the model checkpoint at gpt2 and are newly initialized: ['h.0.attn.masked_bias', 'h.1.attn.masked_bias', 'h.2.attn.masked_bias', 'h.3.attn.masked_bias', 'h.4.attn.masked_bias', 'h.5.attn.masked_bias', 'h.6.attn.masked_bias', 'h.7.attn.masked_bias', 'h.8.attn.masked_bias', 'h.9.attn.masked_bias', 'h.10.attn.masked_bias', 'h.11.attn.masked_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Foreigners have taken a particular interest in otaku culture. A large number of them have been writing in the language or using online platforms, such as the Internet or the video game community, for years. It isn't just a matter of the internet\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2')\n",
    "set_seed(42)\n",
    "\n",
    "response = generator(\"Foreigners have taken a particular interest in otaku culture.\", max_length=50, \n",
    "                     num_return_sequences=1, early_stopping=True, length_penalty=0.01)\n",
    "\n",
    "for i, item in enumerate(response):\n",
    "    print(i)\n",
    "    print(item['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD = \"I DON’T do it for the money. I’ve got enough, much more than I’ll ever need. I do it to do it. Deals are my art form. Other people paint beautifully on canvas or write wonderful poetry. I like making deals, preferably big deals. That’s how I get my kicks. Most people are surprised by the way I work. I play it very loose. I don’t carry a briefcase. I try not to schedule too many meetings. I leave my door open. You can’t be imaginative or entrepreneurial if you’ve got too much structure. I prefer to come to work each day and just see what develops. There is no typical week in my life. I wake up most mornings very early, around six, and spend the first hour or so of each day reading the morning newspapers. I usually arrive at my office by nine, and I get on the phone. There’s rarely a day with fewer than fifty calls, and often it runs to over a hundred. In between, I have at least a dozen meetings. The majority occur on the spur of the moment, and few of them last longer than fifteen minutes. I rarely stop for lunch. I leave my office by six-thirty, but I frequently make calls from home until midnight, and all weekend long. It never stops, and I wouldn’t have it any other way. I try to learn from the past, but I plan for the future by focusing exclusively on the present. That’s where the fun is. And if it can’t be fun, what’s the point?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "#tokenizer.add_special_tokens({'pad_token': f'[{PAD}]'})\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': f'[{PAD}]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers.data.processors.utils import InputFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetDataset(Dataset):\n",
    "    def __init__(self, tweets, tokenizer, max_len):\n",
    "        self.tweets,  self.tokenizer, self.max_len = tweets.to_numpy(), tokenizer, max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tweets)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        tweet = self.tweets[item]\n",
    "        tokens = self.tokenizer.encode_plus(\n",
    "            tweet,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "        return InputFeatures(input_ids = tokens['input_ids'].flatten().long().numpy().tolist(), \n",
    "                             attention_mask=tokens['attention_mask'].flatten().long().numpy().tolist())\n",
    "    \n",
    "def create_dataset(df, tokenizer, max_length, batch_size):\n",
    "    return TweetDataset(df, tokenizer, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df['text'], test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = create_dataset(train_df, tokenizer, 780, 4)\n",
    "eval_dataset = create_dataset(val_df, tokenizer, 780, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GPT2Model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-8b7b9f6cf8d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m training_args = TrainingArguments(\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GPT2Model' is not defined"
     ]
    }
   ],
   "source": [
    "model = GPT2Model.from_pretrained(\"gpt2\")\n",
    "for param in model.base_model.parameters():\n",
    "    param.requires_grad = False\n",
    "    \n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total # of training epochs\n",
    "    per_device_train_batch_size=16,  # batch size per device during training\n",
    "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=test_dataset            # evaluation dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 0 ns, total: 2 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:966: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelWithLMHead.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df['text'], test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50913,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = list(train_df)\n",
    "val_df = list(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "train_mod_path = \"train.txt\"\n",
    "test_mod_path = \"test.txt\"\n",
    "\n",
    "\n",
    "train_m = \"\"\n",
    "for tweet in train_df:\n",
    "    train_m += (tokenizer.special_tokens_map['bos_token']+tweet.rstrip()+tokenizer.special_tokens_map['eos_token'])\n",
    "          \n",
    "with open(train_mod_path, \"w\", encoding='utf-8') as f:\n",
    "    f.write(train_m)\n",
    "\n",
    "test_m = \"\"\n",
    "for tweet in val_df:\n",
    "    test_m += (tokenizer.special_tokens_map['bos_token']+tweet.rstrip()+tokenizer.special_tokens_map['eos_token'])\n",
    "        \n",
    "with open(test_mod_path, \"w\", encoding='utf-8') as f:\n",
    "    f.write(test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset, DataCollatorForLanguageModeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128)\n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset,test_dataset,data_collator = load_dataset(train_mod_path,test_mod_path,tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./storage/gpt2-motivational_v6\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=2, # number of training epochs\n",
    "    logging_steps = 500, # Number of update steps between two evaluations.\n",
    "    save_steps=500, # after # steps model is saved\n",
    "    warmup_steps=500,# number of warmup steps for learning rate scheduler\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "        </style>\n",
       "      \n",
       "      <progress value='2' max='3252' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [   2/3252 : < :, Epoch 0.00/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-6ab7d9d77096>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./storage/model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./storage/model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m                         \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m                     \u001b[0mtr_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_total_flos\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloating_point_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    " \n",
    "trainer.save_model(\"./storage/model\")\n",
    " \n",
    "tokenizer.save_pretrained(\"./storage/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/transformers/models/auto/modeling_auto.py:966: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"./storage/model\")\n",
    "model = AutoModelWithLMHead.from_pretrained(\"./storage/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Fed is having problems with China but I think they should just give us free money as a country – and they can fix them as quickly as possible\n",
      "\n",
      "\"Why in the world did the “dummy” Mueller team get so damn fired up after having done so much research and investigating for a year, that they don't know any better?\"\"Because they didn’t even look\n",
      "\n",
      "RT @RepMarkMeadows: On the flipside, a new round of impeachment hearings is now underway with more allegations of…\n",
      "\n",
      "A little over a year after the Election, let’s talk about the Election!\n",
      "\n",
      "You may not get a response right away. You will be waiting…\n",
      "\n",
      "My Administration has made it a priority to get our great Navy &amp; Coast Guard to stay safely &amp; safely in our Southern Border. There really is no doubt!\n",
      "\n",
      "We’ve just watched the destruction and destruction by ISIS of the wonderful @BastilleRoyal in Nice, France-\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".@OANN was on to talk with @realDonaldTrump about the great @VanityFair- http://t.co/HVdHl0dwVm\n"
     ]
    }
   ],
   "source": [
    "response = test(\"\", num_return_sequences=10)\n",
    "for i, item in enumerate(response):\n",
    "    print()\n",
    "    print(item['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tremendous job numbers in Alabama, including a record number of jobs. @FoxBusiness\n",
      "\n",
      "\n",
      "\n",
      "\"@greta @realDonaldTrump \"\"You're fired.\"\" I love him, I like the fact that he speaks a lot of Chinese.\"\"\"\n",
      "\n",
      "We will be holding a major news conference at 10:00 A.M. Eastern (Eastern), shortly!\n",
      "\n",
      "Criminals and their allies would be in the best position to make big gains by stealing our guns from us.” @BreitbartNews\n",
      "\n",
      "Totally unrelated to the @FoxNews Polls.\n",
      "\n",
      "It has taken almost two decades for the U.S. to negotiate trade deals that are meaningful and long overdue. We will now be getting a stronger deal. @TrumpGulfUSMCA\n"
     ]
    }
   ],
   "source": [
    "response = test(\"\", num_return_sequences=10, top_p=0.92)\n",
    "for i, item in enumerate(response):\n",
    "    print()\n",
    "    print(item['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPECIAL_TOKENS = [\"<bos>\", \"<eos>\", \"<speaker1>\", \"<speaker2>\", \"<pad>\"]\n",
    "SPECIAL_TOKENS = ['<RT>', '<qstart>', '<qend>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "foo = {'shape': 'square', 'color': 'blue'}\n",
    "print(type(foo['shape']))\n",
    "print(type(foo.get('shape')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "shuffle() missing 1 required positional argument: 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2c4042212548>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: shuffle() missing 1 required positional argument: 'x'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.shuffle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
